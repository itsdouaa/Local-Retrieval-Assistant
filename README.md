# Local-Retrieval-Assistant

This is a Retrieval-Augmented Generation (RAG) system built in Python, designed to answer user questions by retrieving relevant context from a database of documents and generating answers using a Large Language Model (LLAMA 4 via Groq's API).

## âœ¨ Features

- **Multilingual Support**: Works seamlessly with Arabic, English, and French content
- **Multiple File Format Support**: Process PDF, DOCX, TXT, DOC, and images (with OCR capabilities)
- **Semantic Search**: Advanced embedding-based retrieval using FAISS
- **Easy-to-Use Interface**: Simple command-line interaction
- **Secure API Handling**: Protected API key management
- **Automatic Tagging**: Intelligent keyword extraction from content
- **Cross-Platform**: Available for Fedora, Ubuntu, and Windows

## ğŸ“ Project Structure


ğŸ“¦ Installation

Prerequisites and Setup
See setup_guide.txt


ğŸš€ Usage

After running the setup script for your platform, navigate to the src/ folder and run:

python engine.py

Example interaction:
---------------------------------------------------------------------
Ask Your Question: 
What are the main benefits of artificial intelligence?

Do you want to add some context/files?
yes  # (opens a file selection dialog)

... (system processes files and provides response)
---------------------------------------------------------------------


ğŸ“ Project Structure

---------------------------------------------------------------------

local_retrieval_assistant/

â”œâ”€â”€ Fedora/ # Fedora Linux version

â”‚ â”œâ”€â”€ scripts/ # Fedora-specific setup scripts

â”‚ â””â”€â”€ src/ # Fedora-specific source code

â”œâ”€â”€ Ubuntu/ # Ubuntu/Debian version

â”‚ â”œâ”€â”€ scripts/ # Ubuntu-specific setup scripts

â”‚ â””â”€â”€ src/ # Ubuntu-specific source code

â”œâ”€â”€ Windows/ # Windows version

â”‚ â”œâ”€â”€ scripts/ # Windows-specific setup scripts

â”‚ â””â”€â”€ src/ # Windows-specific source code

â”œâ”€â”€ setup_guide.txt # General setup instructions

â””â”€â”€ test.db # Shared database file

---------------------------------------------------------------------


ğŸ”§ Platform-Specific Source Code

Each operating system has its own optimized version in the respective folder:

Fedora/src/: Linux-optimized code for Fedora systems

Ubuntu/src/: Linux-optimized code for Ubuntu/Debian systems

Windows/src/: Windows-optimized code with proper path handling

âš™ï¸ Configuration

Customizing Behavior
You can customize various aspects of the system:

Modify stop words - Edit stopwords.txt to add/remove words to ignore during tagging

Change embedding model - Update the model in context.py (currently using all-MiniLM-L6-v2)

Adjust retrieval settings - Modify the number of retrieved results in context.py (currently 3)


ğŸ“‹ Core Components (Each Platform)

---------------------------------------------------------------------

src/

â”œâ”€â”€ db.py              # Database management and operations

â”œâ”€â”€ groq_key.py        # Secure API key retrieval

â”œâ”€â”€ groq_API.py        # Groq API integration

â”œâ”€â”€ context.py         # Context management & semantic retrieval

â”œâ”€â”€ file_to_dict.py    # File processing for various formats

â”œâ”€â”€ file_loader.py     # GUI file selection dialog

â”œâ”€â”€ Tags.py            # Automated keyword/tag generation

â”œâ”€â”€ stopwords.txt      # Multilingual stop words list

â”œâ”€â”€ engine.py          # Main application engine

â””â”€â”€ requirements.txt   # Python dependencies


---------------------------------------------------------------------


ğŸ“ Supported File Formats

Text: .txt

Documents: .docx, .doc

PDFs: .pdf

Images: .jpg, .jpeg, .png (with OCR text extraction)


ğŸ› ï¸ Technical Details

Core Technologies

Language Model: LLAMA 4 Scout 17B via Groq API

Embeddings: sentence-transformers/all-MiniLM-L6-v2

Vector Search: FAISS for efficient similarity search

Database: SQLite for content storage

OCR: Tesseract for text extraction from images

File Processing: PyMuPDF (PDFs), python-docx (DOCX), Mammoth (DOC)


ğŸ”§ How It Works

Knowledge Base Construction: Users add documents through a graphical file dialog

Content Processing: Text is extracted, tagged, and stored in the database with embeddings

Query Processing: User questions are converted to embeddings and compared with stored content

Context Retrieval: The most relevant content is retrieved using semantic similarity

Response Generation: LLAMA 4 generates answers based on the retrieved context


ğŸ“„ License

This project is licensed under the MIT License. See the LICENSE file for details.


ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

Fork the project

Create your feature branch (git checkout -b feature/AmazingFeature)

Commit your changes (git commit -m 'Add some AmazingFeature')

Push to the branch (git push origin feature/AmazingFeature)

Open a Pull Request


ğŸ“ Support

If you encounter any issues or have questions:

Check the existing issues

Create a new issue with detailed information about your problem

Include steps to reproduce, expected behavior, and actual behavior


ğŸš€ Future Enhancements

Potential improvements for future versions:

Web interface for easier interaction

Additional file format support

Batch processing of multiple files

Export functionality for conversations

User authentication and knowledge base separation

Enhanced multilingual support for more languages

---------------------------------------------------------------------

Note: This application requires an internet connection to access the Groq API for processing queries.


